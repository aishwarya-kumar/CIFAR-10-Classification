{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf95948",
   "metadata": {},
   "source": [
    "# DS5220 Supervised Machine Learning - Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116240a3",
   "metadata": {},
   "source": [
    "Aishwarya Abbimutt Nagendra Kumar, Keerthana Velilani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0402e831",
   "metadata": {},
   "source": [
    "# Classification on CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb28ac",
   "metadata": {},
   "source": [
    "This project will focus on implementing different classification algorithms on the CIFAR-10 dataset.\n",
    "We are going to use Python for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef1ef3",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916184f5",
   "metadata": {},
   "source": [
    "The data used for this project is the CIFAR-10 dataset. \n",
    "It is an image dataset that contains 60000 images belonging to 10 classes. There are 50000 training images and 10000 test images.\n",
    "Each image is 32x32x3, i.e. 32x32 pixels colour images(3 channels - RGB). \n",
    "The 10 image classes are:\n",
    " * Airplane\n",
    " * Automobile \n",
    " * Bird\n",
    " * Cat\n",
    " * Deer\n",
    " * Dog\n",
    " * Frog\n",
    " * Horse\n",
    " * Ship\n",
    " * Truck\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fea478ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from sklearn.metrics import *  \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc63d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data\n",
    "\n",
    "# define num_class\n",
    "num_classes = 10\n",
    "classesName = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# load dataset keras will download cifar-10 datset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d69b1e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f61ed8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) train shape\n",
      "(10000, 3072) test shape\n",
      "(50000,) train shape\n",
      "(10000,) test shape\n"
     ]
    }
   ],
   "source": [
    "# Converting the 50000 , 32*32*3 images into 50000 * 3072 arrays\n",
    "x_train = x_train.reshape(50000, 3072)\n",
    "x_test = x_test.reshape(10000, 3072)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# normalize the datasets\n",
    "x_train /= 255.\n",
    "x_test /= 255.\n",
    "\n",
    "print(x_train.shape , \"train shape\")\n",
    "print(x_test.shape , \"test shape\")\n",
    "print(y_train.shape , \"train shape\")\n",
    "print(y_test.shape , \"test shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213b3b0",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc05b43",
   "metadata": {},
   "source": [
    "The first model we choose to run is Logistic regression, we will use this as a baseline.\n",
    "Logistic regression is normally used for binary classification tasks, but we can use it for multi-class classification as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3064db75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.405 (0.007)\n",
      "Logistic reg time taken: 112.40054035186768 secs\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 3-fold cross-validation\n",
    "cv = KFold(n_splits=3, random_state=1, shuffle=True)\n",
    "\n",
    "# logistic regression model\n",
    "log_clf = LogisticRegression()\n",
    "\n",
    "# Evaluating the model using cross-validation\n",
    "scores = cross_val_score(log_clf, x_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# model performance\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "end= time.time()\n",
    "print(\"Logistic reg time taken:\", end-start, \"secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eec54e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aish0\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_model = log_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a02dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4051"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on test data\n",
    "y_hat_log = log_clf.predict(x_test)\n",
    "accuracy_score(y_test, y_hat_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d7bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.48      0.45      1000\n",
      "           1       0.47      0.49      0.48      1000\n",
      "           2       0.32      0.28      0.30      1000\n",
      "           3       0.31      0.23      0.27      1000\n",
      "           4       0.36      0.29      0.32      1000\n",
      "           5       0.33      0.37      0.35      1000\n",
      "           6       0.42      0.49      0.45      1000\n",
      "           7       0.46      0.44      0.45      1000\n",
      "           8       0.47      0.52      0.49      1000\n",
      "           9       0.43      0.46      0.44      1000\n",
      "\n",
      "    accuracy                           0.41     10000\n",
      "   macro avg       0.40      0.41      0.40     10000\n",
      "weighted avg       0.40      0.41      0.40     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "print(classification_report(y_test, y_hat_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e70c7a",
   "metadata": {},
   "source": [
    "The logistic model does not seem to converge since max_iterations in th default lbfgs solver is 100. We can try using a different optimizer(solver) and higher iterations in this case. Here we are using the \"saga\" optimizer which is a version of stochastic average gradient descent to optmize the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb4a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic reg with saga time taken: 1518.241302728653 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aish0\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "log_clf2= LogisticRegression(solver='saga')\n",
    "log_model2 = log_clf2.fit(x_train, y_train)\n",
    "\n",
    "end= time.time()\n",
    "print(\"Logistic reg with saga time taken:\", end-start, \"secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b78db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4033"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on test data\n",
    "y_hat_log2 = log_clf2.predict(x_test)\n",
    "accuracy_score(y_test, y_hat_log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46a91409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.49      0.47      1000\n",
      "           1       0.47      0.47      0.47      1000\n",
      "           2       0.32      0.29      0.31      1000\n",
      "           3       0.28      0.26      0.27      1000\n",
      "           4       0.35      0.29      0.32      1000\n",
      "           5       0.33      0.33      0.33      1000\n",
      "           6       0.40      0.46      0.43      1000\n",
      "           7       0.45      0.44      0.44      1000\n",
      "           8       0.50      0.53      0.52      1000\n",
      "           9       0.43      0.46      0.45      1000\n",
      "\n",
      "    accuracy                           0.40     10000\n",
      "   macro avg       0.40      0.40      0.40     10000\n",
      "weighted avg       0.40      0.40      0.40     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "print(classification_report(y_test, y_hat_log2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d4059",
   "metadata": {},
   "source": [
    "As expected logistic regression does not perform very well on image classification task. Even with a \"saga\" classifier it was not able to converge properly. Logistic regression is a linear classification model that works by fitting a linear decision boundary to separate the classes. However, image classification tasks typically involve highly non-linear and complex relationships between the image pixels and the corresponding labels. Logistic regression is not capable of capturing these complex relationships, and therefore may not perform well on image classification tasks.\n",
    "\n",
    "This model establishes as a baseline for the rest of algorithms that we can try. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192b631",
   "metadata": {},
   "source": [
    "### Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da64b8",
   "metadata": {},
   "source": [
    "A random forest model is an ensemble learning method that combines multiple decision trees to improve predictive accuracy and reduce overfitting.\n",
    "\n",
    "The basic idea is to create a large number of decision trees, each trained on a random subset of the data and a random subset of the features. The final prediction is then obtained by averaging the predictions of all the individual trees in the forest.\n",
    "\n",
    "The randomness introduced in the training process helps to create diverse trees, which can better capture the complex relationships between the features and the target variable. This makes the model less prone to overfitting and more robust to noisy data.\n",
    "\n",
    "During prediction, each tree in the forest independently generates a prediction, and the final prediction is obtained by aggregating the results from all the trees. The algorithm is particularly effective for high-dimensional data and can handle both regression and classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b8f3e",
   "metadata": {},
   "source": [
    "#### Why Random forest?\n",
    "\n",
    "Random forest can capture non-linear relationships between the input features and the output classes, while logistic regression assumes a linear relationship between the input features and the output classes. Non-linear relationships are common in image classification tasks, where the relationship between the pixel values and the class labels may be highly complex.\n",
    "\n",
    "Random forest uses an ensemble of decision trees to make predictions, which can help to reduce overfitting and improve generalization performance. Unlike logistic regression, random forest is also robust to noise and outliers in the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ffe193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Validation accuracy: 0.4737\n",
      "RF with default hyperparams, time taken: 555.1321930885315 secs\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Define random forest classifier with default hyperparameters\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Train the random forest classifier\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "# Predict on training and validation sets\n",
    "y_train_pred_rf = rf.predict(x_train)\n",
    "y_test_pred_rf = rf.predict(x_test)\n",
    "\n",
    "# Calculate accuracy on training and validation sets\n",
    "train_acc = accuracy_score(y_train, y_train_pred_rf)\n",
    "val_acc = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print('Training accuracy:', train_acc)\n",
    "print('Validation accuracy:', val_acc)\n",
    "\n",
    "end= time.time()\n",
    "print(\"RF with default hyperparams, time taken:\", end-start, \"secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ac6e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4737"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy score\n",
    "accuracy_score(y_test, y_test_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef884119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56      1000\n",
      "           1       0.51      0.56      0.54      1000\n",
      "           2       0.38      0.32      0.35      1000\n",
      "           3       0.34      0.29      0.31      1000\n",
      "           4       0.39      0.41      0.40      1000\n",
      "           5       0.43      0.39      0.41      1000\n",
      "           6       0.49      0.58      0.53      1000\n",
      "           7       0.52      0.46      0.49      1000\n",
      "           8       0.59      0.61      0.60      1000\n",
      "           9       0.49      0.55      0.52      1000\n",
      "\n",
      "    accuracy                           0.47     10000\n",
      "   macro avg       0.47      0.47      0.47     10000\n",
      "weighted avg       0.47      0.47      0.47     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "print(classification_report(y_test, y_test_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40505c89",
   "metadata": {},
   "source": [
    "With default values for hyperparamters, the random forest classifier was able to give an accuracy of 47% on the test data. This is a poor model, which performs worse than chance. The reson behind this could be the inability of RF to clearly read the features in the image data. Image data is extremely complex, and since we do not have a lot of pre-made features that can help the algorithm understand and classify the data into respective classes, RF model can't work well on this data either. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7f2db",
   "metadata": {},
   "source": [
    "#### RF with hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a08ff",
   "metadata": {},
   "source": [
    "Random forest models require hyperparameter tuning because there are several hyperparameters that can significantly affect the performance of the model. Some of the important hyperparameters include the number of trees in the forest, the depth of the trees, the number of features used to split each node, and the minimum number of samples required to split a node.\n",
    "\n",
    "Tuning the hyperparameters is important because different values can have a significant impact on the performance of the model, and choosing the wrong values can result in poor performance or overfitting.\n",
    "\n",
    "We are using grid search to systematically go over the grid of hyperparamter values and choose the best estimator which has highest accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f4a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid search parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Create random forest model\n",
    "rf_model2 = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model2, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa6eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model on test set\n",
    "y_pred_rf2 = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eca4d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #classification report\n",
    "# print(classification_report(y_test, y_pred_rf2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b5eed",
   "metadata": {},
   "source": [
    "Even with tuning of hyperparamters, RF model is yielding a low accuracy. This could mean that the image data is too complex for random forest algorithm and hence we require a much more complex approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c155089",
   "metadata": {},
   "source": [
    "## Deep neural network, without convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c519d5e",
   "metadata": {},
   "source": [
    "Image classification needs algorithms which are much more complex and can handle the complexity and non-linearity in image data.\n",
    "\n",
    "A dense neural network, also known as a multi-layer perceptron, is a type of artificial neural network that consists of multiple layers of interconnected nodes or neurons. Each neuron in a dense neural network receives input from the neurons in the previous layer, and outputs a value that is computed using an activation function. The layers between the input and output layers are called hidden layers, and the number of neurons in each layer can be adjusted to optimize the performance of the network.\n",
    "\n",
    "We have tried to use a dense neural network -specifically a feedforward neural network with 6 dense layers and the final layer being a softmax activation layer. The first five layers each have a ReLU activation function, which is a common choice for neural networks. The number of neurons in each layer progressively decreases. The last layer has 10 neurons, corresponding to the number of classes in the classification task, and uses a softmax activation function to output probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf0d6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(600, activation=\"relu\"))\n",
    "model.add(Dense(400, activation=\"relu\"))\n",
    "model.add(Dense(200, activation=\"relu\"))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))  #Last layer with number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ca8ca",
   "metadata": {},
   "source": [
    "The model summary is as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0887b0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 600)               1843800   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 400)               240400    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 200)               80200     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,190,060\n",
      "Trainable params: 2,190,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd0f11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complie the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb15cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n",
      "Epoch 11/50\n",
      "Epoch 12/50\n",
      "Epoch 13/50\n",
      "Epoch 14/50\n",
      "Epoch 15/50\n",
      "Epoch 16/50\n",
      "Epoch 17/50\n",
      "Epoch 18/50\n",
      "Epoch 19/50\n",
      "Epoch 20/50\n",
      "Epoch 21/50\n",
      "Epoch 22/50\n",
      "Epoch 23/50\n",
      "Epoch 24/50\n",
      "Epoch 25/50\n",
      "Epoch 26/50\n",
      "Epoch 27/50\n",
      "Epoch 28/50\n",
      "Epoch 29/50\n",
      "Epoch 30/50\n",
      "Epoch 31/50\n",
      "Epoch 32/50\n",
      "Epoch 33/50\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, verbose=3)\n",
    "\n",
    "end= time.time()\n",
    "print(\"RF with default hyperparams, time taken:\", end-start, \"secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53f5b274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_dnn = model.predict(x_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdc777be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4802"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ed8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_dnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0574b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc      = history.history['accuracy']\n",
    "val_acc  = history.history['val_accuracy']\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs   = range(len(acc)) # Get number of epochs\n",
    "\n",
    "# Plot training and validation accuracy per epoch\n",
    "plt.plot  ( epochs,     acc )\n",
    "plt.plot  ( epochs, val_acc )\n",
    "plt.title ('Training and validation accuracy')\n",
    "plt.figure()\n",
    "\n",
    "# Plot training and validation loss per epoch\n",
    "plt.plot  ( epochs,     loss )\n",
    "plt.plot  ( epochs, val_loss )\n",
    "plt.title ('Training and validation loss')\n",
    "plt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fabea42",
   "metadata": {},
   "source": [
    "From the above results we can clearly see that the neural network is overfitting on the training data. Perhaps, with a bit more of trial and error on the architecture of the NN and with hyperparamter tuning, we can improve the accuracy. But we will go ahead and try using convolutional layers in the network, since convolutions prove to be much more effective in image classification. \n",
    "\n",
    "Convolutions are helpful for image classification because they are able to capture local patterns and features in an image in a way that is translation invariant. In other words, convolutions can detect the same patterns regardless of their location in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559e5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
